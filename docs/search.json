[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "EPPS 6323.001 Project Proposal",
    "section": "",
    "text": "RESEARCH STATEMENT:\nOur aim is to leverage knowledge-mining techniques to understand digitalization practices within government institutions. Specifically, in a data-driven modern society, the protection of citizen data is a growing concern. Policy interventions such as the Federal Data Breach Disclosure Law have emerged as a tool to ensure transparency and better governance of personal data. Inspired by Gay’s (2017) and Sullivan’s (2010) studies, this research study will look at the public sector to ascertain the strength of correlations between select financial metrics derived from the Electronic Municipal Market Access (EMMA) dataset and whether a city government was hacked or not (i.e. a binary dependent variable) which we will pull from various state-wide disclosures. If we find statistically significant relationships via our slated approaches, we may further analyze the effects of cyberattacks on the overall financial performance of the affected entities.\n\n\nBACKGROUND & IMPACT - WHY IS THIS IMPORTANT?\nAccording to the Privacy Rights Org 2023 report, there have been 20,030 data breaches in the U.S. since 2005 that leak out 1,993,415,481 impacted records. And according to CompariTech, U.S. schools had at least 2,691 data breaches that leaked 32 million records! The main sources of breaches are hacking, ransomware attacks, and third-party breaches such as the Illuminate Breach incident in 2021 to 2022, affecting 605 educational institutions with 2.1 million individual records being compromised. This means an individual’s personally identifiable information (PII), such as name, full social security number, date of birth, address, ID numbers, medical history, banking information, and other undisclosed records, are compromised or stolen. Such events victimize individuals and put them at risk for identification frauds and financial frauds. Billions of compromised individual records, with an average cost of $9.44 million per breach, have posed and continue to pose a great economic burden on both the private and public sectors.\n\n\nCONTRIBUTION & CURRENT LITERATURE:\nData breach incidents from 2005 to 2010 indicate governments experience the second-highest share of compromised data records. Yet, there are few studies that analyze the characteristics of governments that are targets of cyber-attacks. Current literature on cyber breaches mainly concentrates on the private sector.The only two papers studying the effect of municipal cyber breaches on municipal bonds show conflicting findings on bond yields. Our aim is to better understand the factors that contribute to the likelihood of government entities becoming the target of cyberattacks, of which there can be many, including but not limited to party affiliation, financial structure, and scope of jurisdiction. We hope to take this research further and analyze the effect of cyberattacks on the financial performance of the entity using municipal bond data as a reference.\n\n\nMETHODOLOGY & METHODS:\nFinancial data will be derived from the Electronic Municipal Market Access (EMMA) dataset, a product of the Municipal Securities Rulemaking Board (MSRB). Breach data will be acquired from various state-wide disclosures. We plan to use unsupervised methods such as K-Means Clustering, Hierarchical Clustering, and Principal Components Analysis (Sohil et al. 2022, ch. 12-13) in order to help tease apart relationships between all our data. We will also use various supervised machine learning methods such as logistic, K-nearest neighbor, decision tree-based, and support vector machines classification methods, along with polynomial and spline regression (Sohil et al. 2022, ch. 4-9) in order to find information concerning any structured relationships between these variables. Specifically using the fact of a local government experiencing a breach or not as our dependent variable. Additionally, we plan on using re-sampling methods such as cross-validation and bootstrapping (Sohil et al. 2022, ch. 5) to test the strength of various approaches. This research will seek to contribute to policy analysis more generally by identifying patterns and correlations that elucidate the evolving role of digitalization in the context of modern governance. Of course, the results of our findings will be limited to the locales we study and not directly generalizable to other cities, states, nations, etc.\n\n\nDATA COLLECTION & SOURCES\nData Source: o https://debtsearch.brb.texas.gov/ o https://comptroller.texas.gov/transparency/open-data/cpa-databases/ o MSRB | MSRB - Municipal Securities Rulemaking Board::EMMA (msrb.org) o FINRA Data\n\n\nREFERENCES\nGay, Sebastien. 2017. “Strategic News Bundling and Privacy Breach Disclosures.” Journal of Cybersecurity 3(2): 91–108. doi:10.1093/cybsec/tyx009. Morgan Lewis. “Study Finds Average Cost of Data Breaches Reaches All-Time High in 2022.” https://www.morganlewis.com/blogs/sourcingatmorganlewis/2023/01/study-findsaverage-cost-of-data-breaches-reaches-all-time-high-in-2022 (February 20, 2024). MSRB. “Municipal Securities Rulemaking Board::EMMA.” https://emma.msrb.org/ (February 20, 2024). NCSL. 2022. “Security Breach Notification Laws.” Security Breach Notification Laws. https://www.ncsl.org/technology-and-communication/security-breach-notification-laws# (February 20, 2024). PrivacyRights.org. “United States Data Breach Notification in the United States 2023 Report | Privacy Rights Clearinghouse.” Data Breaches. https://privacyrights.org/resources/united-states-data-breach-notification-united-states2023-report (February 20, 2024). Sohil, Fariha, Muhammad Umair Sohali, and Javid Shabbir. 2022. “An Introduction to Statistical Learning with Applications in R: By Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, New York, Springer Science and Business Media, 2013, $41.98, eISBN: 978-1-4614-7137-7.” Statistical Theory and Related Fields 6(1): 87–87. doi:10.1080/24754269.2021.1980261. Sullivan, Richard J. 2010. “The Changing Nature of U.S. Card Payment Fraud: Industry and Public Policy Options.”"
  },
  {
    "objectID": "Lab03.html",
    "href": "Lab03.html",
    "title": "EPPS 6323: Lab03 R programming (Exploratory Data Analysis)",
    "section": "",
    "text": "R Programming (EDA)\n\n## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9965738\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9965738\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5 (Lab 4) - Unsupervised learning",
    "section": "",
    "text": "Unsupervised learning involves algorithms that detect patterns or grouping structures in data without relying on labeled information like dependent variables. Instead, it explores the inherent structure and possible groupings of unlabeled data, often used as a pre-processing step for supervised learning.\nIn unsupervised learning, there’s no explicit dependent variable for prediction (Y). The focus is on revealing patterns within measurements (X1, X2, …, Xp) and identifying any underlying subgroups among observations.\nThis section introduces two main methods:\n\n\nPrincipal Components Analysis (PCA) generates a correlated, low-dimensional representation of a dataset by identifying linear combinations of variables with maximal variance and mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] ,\nwhich has the largest variance.\n“Normalized” means that: \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) is the loading of the first principal component.\nThe loadings, collectively, make up the principal compobebt loading “vector” = \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings to set the sum of squares = 1. Otherwise, setting these elements to be arbitrarily large in absolute value might result in an arbitrarily large variance.\n\n\n\n\n\nK-Means Clustering is a method used to divide data points into K groups while minimizing the sum of squares from each point to its assigned cluster center within each group.\n\n\n\nHierarchical clustering offers an alternative approach that doesn’t require a pre-determined or pre-specified or a particular value/choice of \\((K)\\).\nOne advantage of Hierarchical Clustering is its ability to generate a dendrogram - a tree-based depiction of observations.\nThe dendrogram is constructed by merging clusters from the leaves to the trunk. It provides a visual representation of data relationships, allowing for subgroup identification by cutting the dendrogram at desired similarity levels."
  },
  {
    "objectID": "assignment5.html#principal-component-analysis-pca",
    "href": "assignment5.html#principal-component-analysis-pca",
    "title": "Assignment 5 (Lab 4) - Unsupervised learning",
    "section": "",
    "text": "Principal Components Analysis (PCA) generates a correlated, low-dimensional representation of a dataset by identifying linear combinations of variables with maximal variance and mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] ,\nwhich has the largest variance.\n“Normalized” means that: \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) is the loading of the first principal component.\nThe loadings, collectively, make up the principal compobebt loading “vector” = \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings to set the sum of squares = 1. Otherwise, setting these elements to be arbitrarily large in absolute value might result in an arbitrarily large variance."
  },
  {
    "objectID": "assignment5.html#clustering",
    "href": "assignment5.html#clustering",
    "title": "Assignment 5 (Lab 4) - Unsupervised learning",
    "section": "",
    "text": "K-Means Clustering is a method used to divide data points into K groups while minimizing the sum of squares from each point to its assigned cluster center within each group.\n\n\n\nHierarchical clustering offers an alternative approach that doesn’t require a pre-determined or pre-specified or a particular value/choice of \\((K)\\).\nOne advantage of Hierarchical Clustering is its ability to generate a dendrogram - a tree-based depiction of observations.\nThe dendrogram is constructed by merging clusters from the leaves to the trunk. It provides a visual representation of data relationships, allowing for subgroup identification by cutting the dendrogram at desired similarity levels."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1 (& Prep for Class 2)",
    "section": "",
    "text": "Team Project Brainstorm:\n\nResearch Question\nHypothesis\nContribution & Impact\nData Source & Data Collection\nResearch Design\n\nMethods\n\n\nPrep for Class 2 – Comparing Breiman (2001) and Shmueli (2010):\nBreiman (2001) and Shmueli (2010) offer insightful discussions on different methodologies within statistical modeling, each addressing distinct but complementary aspects of the field.\nBreiman’s 2001 article, “Statistical Modeling: The Two Cultures,” contrasts the data model and algorithmic model approaches. He criticizes the data model approach for its reliance on possibly unrealistic assumptions about how data is generated and advocates for the algorithmic model approach, which includes methods like decision trees and neural networks, for its flexibility and ability to manage complex data without predefined assumptions. Breiman calls for integrating both approaches to tackle real-world issues better, emphasizing the necessity of model validation through techniques like cross-validation.\nShmueli (2010), in “To Explain or to Predict?”, differentiates between explanatory and predictive modeling. She challenges the belief that explanatory power implies predictive accuracy, arguing that they fulfill different roles: explanatory modeling tests causal hypotheses, while predictive modeling forecasts future events.\nShmueli defends predictive modeling against academic biases, highlighting its potential to reveal new causal mechanisms and serve as a reality check for causal theories. She outlines four main differences between the two modeling types, including their approaches to causation, theory, focus (retrospective vs. prospective), and the bias-variance dilemma, advocating for a clear distinction and appreciation of both.\nThese two articles advocate for a broader, more inclusive approach to statistical modeling. Breiman (2001) focuses on the methodological divide and the potential of algorithmic models for real-world data, whereas Shmueli (2010) emphasizes the distinct but complementary roles of explanatory and predictive modeling. Both call for flexibility, rigorous validation, and a balanced use of modeling techniques to advance theoretical and practical applications in the field."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "",
    "text": "library(haven) # For reading Stata data files  \n\nWarning: package 'haven' was built under R version 4.3.2\n\nlibrary(MASS)  # For ordinal logistic regression  \n\nWarning: package 'MASS' was built under R version 4.3.2\n\nlibrary(vcd)   # For categorical data  \n\nWarning: package 'vcd' was built under R version 4.3.2\n\n\nLoading required package: grid"
  },
  {
    "objectID": "assignment2.html#load-necessary-libraries",
    "href": "assignment2.html#load-necessary-libraries",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "",
    "text": "library(haven) # For reading Stata data files  \n\nWarning: package 'haven' was built under R version 4.3.2\n\nlibrary(MASS)  # For ordinal logistic regression  \n\nWarning: package 'MASS' was built under R version 4.3.2\n\nlibrary(vcd)   # For categorical data  \n\nWarning: package 'vcd' was built under R version 4.3.2\n\n\nLoading required package: grid"
  },
  {
    "objectID": "assignment2.html#read-the-dataset-into-r",
    "href": "assignment2.html#read-the-dataset-into-r",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Read the dataset into R",
    "text": "Read the dataset into R\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")"
  },
  {
    "objectID": "assignment2.html#data-preprocessing",
    "href": "assignment2.html#data-preprocessing",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nCalculate Missing Values\n\nna_count &lt;- sapply(TEDS_2016, function(y) sum(is.na(y)))  \nna_count_votetsai &lt;- sum(is.na(TEDS_2016$votetsai))  \n\n\n\nExamine Unique Values\n\nunique(TEDS_2016$Tondu)  \n\n&lt;labelled&lt;double&gt;[7]&gt;: Position on unification and independence\n[1] 3 5 9 4 6 2 1\n\nLabels:\n value                                                              label\n     1                                              Immediate unification\n     2                    Maintain the status quo,move toward unification\n     3 Maintain the status quo, decide either unification or independence\n     4                                    Maintain the status quo forever\n     5                   Maintain the status quo,move toward independence\n     6                                             Immediate independence\n     9                                                        Nonresponse\n\nsummary(TEDS_2016$Tondu)  \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   4.000   4.127   5.000   9.000 \n\n\n\n\nHistogram of ‘Tondu’\n\nhist(TEDS_2016$Tondu, main = \"Histogram of Tondu\", xlab = \"Tondu\")  \n\n\n\n\n\n\n\n\n\n\nCheck Binary Distribution\n\ntable(TEDS_2016$female)  \n\n\n  0   1 \n868 822 \n\n\n\n\nConvert Numerics to Factors\n\nTEDS_2016$female &lt;- factor(TEDS_2016$female, levels = c(0, 1), labels = c(\"Male\", \"Female\"))  \nTEDS_2016$DPP &lt;- factor(TEDS_2016$DPP)  \n   \nbinary_columns &lt;- sapply(TEDS_2016, function(x) all(x %in% c(0, 1, NA)))  \nTEDS_2016[, binary_columns] &lt;- lapply(TEDS_2016[, binary_columns], function(x) factor(x, levels = c(0, 1)))  \n\n\n\nImpute Missing Values\n\n# Numerical variables  \nnumerical_columns &lt;- sapply(TEDS_2016, is.numeric) & !binary_columns  \nTEDS_2016[, numerical_columns] &lt;- lapply(TEDS_2016[, numerical_columns], function(x) {  \n  ifelse(is.na(x), mean(x, na.rm = TRUE), x)  \n})  \n   \n# Categorical variables  \ncategorical_columns &lt;- sapply(TEDS_2016, is.factor)  \nTEDS_2016[, categorical_columns] &lt;- lapply(TEDS_2016[, categorical_columns], function(x) {  \n  freq_table &lt;- table(x)  \n  mode_value &lt;- names(which.max(freq_table))  \n  x[is.na(x)] &lt;- mode_value  \n  factor(x, levels = levels(x))  \n})"
  },
  {
    "objectID": "assignment2.html#analysis",
    "href": "assignment2.html#analysis",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Analysis",
    "text": "Analysis\n\nFrequency Table for ‘Tondu_with_no_response’\n\nTEDS_2016$Tondu_numeric &lt;- as.numeric(TEDS_2016$Tondu)\nTEDS_2016$Tondu_with_no_response &lt;- factor(TEDS_2016$Tondu_numeric,  \n                                           levels = c(1, 2, 3, 4, 5, 6, 9),  \n                                           labels = c(\"Immediate unification\",  \n                                                      \"Maintain the status quo, move toward unification\",  \n                                                      \"Maintain the status quo, decide either unification or independence\",  \n                                                      \"Maintain the status quo forever\",  \n                                                      \"Maintain the status quo, move toward independence\",  \n                                                      \"Immediate independence\",  \n                                                      \"No response\"))  \ntondu_freq_table &lt;- table(TEDS_2016$Tondu_with_no_response)  \ntondu_freq_table  \n\n\n                                             Immediate unification \n                                                                27 \n                  Maintain the status quo, move toward unification \n                                                               180 \nMaintain the status quo, decide either unification or independence \n                                                               546 \n                                   Maintain the status quo forever \n                                                               328 \n                 Maintain the status quo, move toward independence \n                                                               380 \n                                            Immediate independence \n                                                               108 \n                                                       No response \n                                                               121 \n\n\n\n\nOrdinal Logistic Regression\n\nTEDS_2016 &lt;- subset(TEDS_2016, !is.na(Tondu) & Tondu != 9 & !is.na(votetsai))  \nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels = c(1, 2, 3, 4, 5, 6),  \n                          labels = c(\"Immediate unification\",  \n                                     \"Maintain the status quo, move toward unification\",  \n                                     \"Maintain the status quo, decide either unification or independence\",  \n                                     \"Maintain the status quo forever\",  \n                                     \"Maintain the status quo, move toward independence\",  \n                                     \"Immediate independence\"),  \n                          ordered = TRUE)  \nordinal_model &lt;- polr(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse,  \n                      data = TEDS_2016)  \n\n\n\nGoodman and Kruskal’s Gamma Coefficient\n\ngamma_age &lt;- assocstats(table(TEDS_2016$age, TEDS_2016$Tondu_numeric))$gamma  \ngamma_income &lt;- assocstats(table(TEDS_2016$income, TEDS_2016$Tondu_numeric))$gamma  \ngamma_edu &lt;- assocstats(table(TEDS_2016$edu, TEDS_2016$Tondu_numeric))$gamma  \ngamma_results &lt;- list(age_gamma = gamma_age, income_gamma = gamma_income, edu_gamma = gamma_edu)  \n\n\n\nChi-Square Test of Independence\n\nTEDS_2016$DPP &lt;- factor(TEDS_2016$DPP)  \nTEDS_2016$Taiwanese &lt;- factor(TEDS_2016$Taiwanese)  \nTEDS_2016$Econ_worse &lt;- factor(TEDS_2016$Econ_worse)  \n   \nchi_square_DPP &lt;- chisq.test(table(TEDS_2016$DPP, TEDS_2016$Tondu), simulate.p.value = TRUE)  \nchi_square_Taiwanese &lt;- chisq.test(table(TEDS_2016$Taiwanese, TEDS_2016$Tondu), simulate.p.value = TRUE)  \nchi_square_Econ_worse &lt;- chisq.test(table(TEDS_2016$Econ_worse, TEDS_2016$Tondu), simulate.p.value = TRUE)  \nchi_square_results &lt;- list(DPP = chi_square_DPP, Taiwanese = chi_square_Taiwanese, Econ_worse = chi_square_Econ_worse)  \n\n\n\nLogistic Regression\n\nTEDS_2016 &lt;- subset(TEDS_2016, !is.na(votetsai))  \nlogistic_model &lt;- glm(votetsai ~ female + DPP + age + income + edu + Taiwanese + Econ_worse,  \n                      family = binomial(link = \"logit\"), data = TEDS_2016)  \nsummary(logistic_model)  \n\n\nCall:\nglm(formula = votetsai ~ female + DPP + age + income + edu + \n    Taiwanese + Econ_worse, family = binomial(link = \"logit\"), \n    data = TEDS_2016)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.490842   0.433296   3.441  0.00058 ***\nfemaleFemale -0.404225   0.133554  -3.027  0.00247 ** \nDPP1          2.963318   0.259074  11.438  &lt; 2e-16 ***\nage          -0.023759   0.005080  -4.677 2.91e-06 ***\nincome       -0.003661   0.025452  -0.144  0.88563    \nedu          -0.175804   0.060354  -2.913  0.00358 ** \nTaiwanese1    1.038279   0.135487   7.663 1.81e-14 ***\nEcon_worse1   0.358628   0.132925   2.698  0.00698 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1898.4  on 1568  degrees of freedom\nResidual deviance: 1398.2  on 1561  degrees of freedom\nAIC: 1414.2\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nMore Chi-Square Tests\n\nchi_square_female &lt;- chisq.test(table(TEDS_2016$female, TEDS_2016$votetsai), simulate.p.value = TRUE)  \nchi_square_DPP &lt;- chisq.test(table(TEDS_2016$DPP, TEDS_2016$votetsai), simulate.p.value = TRUE)  \nchi_square_results_votetsai &lt;- list(female = chi_square_female, DPP = chi_square_DPP)"
  },
  {
    "objectID": "assignment2.html#visualization",
    "href": "assignment2.html#visualization",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Visualization",
    "text": "Visualization\n\nBar Chart for ‘Tondu_with_no_response’\n\nbarplot(tondu_freq_table,  \n        main = \"Frequency of Tondu Categories (Including 'No response')\",  \n        xlab = \"Tondu Categories\",  \n        ylab = \"Frequency\",  \n        las = 2,  \n        col = \"blue\")  \n\n\n\n\n\n\n\n\n\n\nBoxplot of Age by Tondu\n\nboxplot(age ~ Tondu, data = TEDS_2016, main = \"Boxplot of Age by Tondu\", xlab = \"Tondu\", ylab = \"Age\")  \n\n\n\n\n\n\n\n\n\n\nBar Chart of Gender Vote by Tondu\n\nbarplot(table(TEDS_2016$female, TEDS_2016$Tondu_numeric),  \n        beside = TRUE,  \n        legend = c(\"Male\", \"Female\"),  \n        main = \"Bar Chart of Gender Vote by Tondu\",  \n        xlab = \"Tondu\",  \n        ylab = \"Count\",  \n        args.legend = list(title = \"Gender\", x = \"topright\", cex = 0.8))  \n\n\n\n\n\n\n\n\n\n\nBar Chart of Gender by Vote for Tsai\n\nbarplot(table(TEDS_2016$female, TEDS_2016$votetsai),  \n        beside = TRUE,  \n        legend = c(\"Male\", \"Female\"),  \n        main = \"Bar Chart of Gender by Vote for Tsai\",  \n        xlab = \"Vote for Tsai\",  \n        ylab = \"Count\",  \n        args.legend = list(title = \"Gender\", x = \"topright\", cex = 0.8))"
  },
  {
    "objectID": "assignment2.html#results-and-discussion",
    "href": "assignment2.html#results-and-discussion",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Results and Discussion",
    "text": "Results and Discussion"
  },
  {
    "objectID": "assignment2.html#results-and-discussion-1",
    "href": "assignment2.html#results-and-discussion-1",
    "title": "Analysis of TEDS_2016 Dataset",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nAnalysis of the results: - (Intercept): The estimated log-odds of votetsai being 1 (versus 0) when all predictors are at their reference levels is 1.490842. This is statistically significant with a p-value of 0.00058.\n\nfemaleFemale: Being female is associated with a decrease in the log-odds of votetsai by -0.404225 compared to being male (the reference category). This effect is statistically significant (p = 0.00247).\nDPP1: Affiliation with DPP (Democratic Progressive Party) is associated with an increase in the log-odds of votetsai by 2.963318 compared to non-affiliation (the reference category). This is highly statistically significant (p &lt; 2e-16).\nage: Each additional year of age is associated with a decrease in the log-odds of votetsai by -0.023759. This effect is statistically significant (p = 2.91e-06).\nincome: The coefficient for income is not statistically significant (p = 0.88563), suggesting that income does not have a significant effect on the log-odds of votetsai.\nedu: Higher education levels are associated with a decrease in the log-odds of votetsai by -0.175804. This effect is statistically significant (p = 0.00358).\nTaiwanese1: Identifying as Taiwanese is associated with an increase in the log-odds of votetsai by 1.038279. This effect is highly statistically significant (p = 1.81e-14).\nEcon_worse1: Believing the economy has gotten worse is associated with an increase in the log-odds of votetsai by 0.358628. This effect is statistically significant (p = 0.00698).\n\nThe model’s AIC (Akaike Information Criterion) is 1414.2, which can be used for model comparison purposes. The lower the AIC, the better the model fits the data while penalizing for complexity.\nThe null deviance and residual deviance indicate how well the model fits the data compared to a null model with only the intercept. The significant reduction from the null deviance to the residual deviance shows that the predictors improve the model fit.\nOverall, the model suggests that gender, DPP affiliation, age, education level, Taiwanese identity, and perception of the economy are significant predictors of votetsai. Income is not a significant predictor in this model.\n```"
  },
  {
    "objectID": "assignment8.html",
    "href": "assignment8.html",
    "title": "assignment 8",
    "section": "",
    "text": "require(ISLR)\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\nrequire(MASS)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.3.2\n\nrequire(descr)\n\nLoading required package: descr\n\n\nWarning: package 'descr' was built under R version 4.3.3\n\nattach(Smarket)\n\n## Linear Discriminant Analysis\nfreq(Direction)\n\n\n\n\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"lightgreen\")\n\n\n\n\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238\n\n\n\nANSWER 1:\nWhile the best subset selection model may have the smallest training RSS, stepwise selection models, particularly those using test RSS as a criterion, aim to find models that generalize well to unseen data. The choice between forward and backward models depends on the dataset and should be evaluated using appropriate validation techniques.\nBest Subset Selection Model:\nBest subset selection involves fitting all possible combinations of predictors and selecting the model with the lowest training RSS. Since it considers all possible subsets of predictors, it tends to find the model that fits the training data best among all the candidates. However, selecting the best subset based solely on training RSS can lead to overfitting, especially with a large number of predictors, as the model may capture noise in the data.\nStepwise Selection Models:\nStepwise selection methods, such as forward and backward selection, involve adding or removing predictors iteratively based on certain criteria (e.g., AIC, BIC, adjusted R-squared) until a stopping criterion is met.\nWhile stepwise selection methods aim to find a parsimonious model that generalizes well to unseen data, they typically use techniques like cross-validation to estimate the performance of the selected model on test data. Therefore, the model selected through stepwise selection often has the smallest test RSS, indicating better generalization performance compared to the best subset selection model.\nForward vs. Backward Models:\nIn forward selection, predictors are added one by one to the model until no significant improvement in the chosen criterion is observed. This approach may result in a model that is simpler and potentially more interpretable.\nIn backward elimination, all predictors are initially included in the model, and one by one, the least significant predictors are removed until no further improvement is observed. This method may lead to a more parsimonious model, especially when dealing with a large number of predictors.\nThe choice between forward and backward selection may vary depending on the dataset characteristics, such as the correlation between predictors and the signal-to-noise ratio. It’s essential to assess the performance of both methods through cross-validation or other validation techniques to determine which one yields the best test RSS for the specific dataset.\n\nset.seed(123)\nx &lt;- rnorm(100)\neps &lt;- rnorm(100)\n\ny &lt;- 4 + 9 * x + 2 * x^2 + x^3 + eps\n\nplot(x)\n\n\n\n\n\n\n\nplot(y)\n\n\n\n\n\n\n\nrequire(leaps)\n\nLoading required package: leaps\n\n\nWarning: package 'leaps' was built under R version 4.3.3\n\nbest_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10))\nbic &lt;- summary(best_subset)$bic\ncp &lt;- summary(best_subset)$cp\nadjr2 &lt;- summary(best_subset)$adjr2\n\nplot(bic, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(cp, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(adjr2, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(bic)\n\n[1] 3\n\nwhich.min(cp)\n\n[1] 3\n\nwhich.max(adjr2)\n\n[1] 7\n\ncoef(best_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n             3.970394              8.920446              1.908457 \npoly(x, 10, raw = T)3 \n             1.020436 \n\n\n\n\nAnswer 2:\nModel 3 is the best as it has the lowest BIC and Cp values. However, Model 7, with the highest adjusted R squared value, might be overfitting as the R squared plot levels off at Model 3. In Model 3, the coefficients are: intercept (3.97), B1 (8.92), B2 (1.91), and B3 (1.02).\n\nfor_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10), method = \"forward\")\n\nplot(summary(for_subset)$bic, type = \"b\", pch = 16, col = \"orange\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(for_subset)$cp, type = \"b\", pch = 16, col = \"pink\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(for_subset)$adjr2, type = \"b\", pch = 16, col = \"steelblue\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(summary(for_subset)$bic)\n\n[1] 3\n\nwhich.min(summary(for_subset)$cp)\n\n[1] 3\n\nwhich.max(summary(for_subset)$adjr2)\n\n[1] 4\n\ncoef(for_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n             3.970394              8.920446              1.908457 \npoly(x, 10, raw = T)3 \n             1.020436 \n\n\nAnswer 3\nModel 3 is the best again, having the lowest BIC and Cp values. While Model 4 has the highest adjusted R squared value, Model 3 shows a similar value in the plot. In Model 3, the coefficients are: intercept (3.97), B1 (8.92), B2 (1.91), and B3 (1.02).\n\nbac_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10), method = \"backward\")\n\nplot(summary(bac_subset)$bic, type = \"b\", pch = 16, col = \"cyan\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(bac_subset)$cp, type = \"b\", pch = 16, col = \"lightgreen\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(bac_subset)$adjr2, type = \"b\", pch = 16, col = \"magenta\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(summary(bac_subset)$bic)\n\n[1] 4\n\nwhich.min(summary(bac_subset)$cp)\n\n[1] 4\n\nwhich.max(summary(bac_subset)$adjr2)\n\n[1] 4\n\ncoef(bac_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n            3.9620068             9.8934015             1.9689642 \npoly(x, 10, raw = T)5 \n            0.1748705 \n\n\n\n\nAnswer 3:\nModel 4, using backwards stepwise selection, outperforms others across all three metrics. The coefficients are as follows: intercept (3.96), B1 (9.89), B2 (1.97), and B3 (0.17)."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Stefanie@utdallas.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome To Stefaniverse! :)",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/wy521/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nWarning: package 'MASS' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wy521\\AppData\\Local\\Temp\\RtmpwBoO5r\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.3.2\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/wy521/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nWarning: package 'MASS' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\wy521\\AppData\\Local\\Temp\\RtmpwBoO5r\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.3.2\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Research Question\nContribution & Impact\nData Source & Data Collection\nResearch Design\nMethod"
  }
]